{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7316,"status":"ok","timestamp":1646675167093,"user":{"displayName":"Mauricio Alfaro","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16047939764221462828"},"user_tz":180},"id":"7k90mci76m8o"},"outputs":[],"source":["\"\"\"Unit for training LSTM engines intended for music generation projects\n","We will be using the Drive directory called maeGenerator22\n","\n","\n","Blocks:\n","\n","1. Loading the training data and parameters: \n","X: raw input data\n","X_f: input data already normalized with MinMax and reshaped for the\n","LSTM with the shape: (len(X_f), SEQUENCE_LENGTH,1)\n","Where SEQUENCE_LENGTH is the number of elements of each training sequence\n","(or the len() of each data example)\n","vocab_length = number of classes in the data target for training\n","all_keys_vocabulary: list of all elements from the vocabulary\n","\n","These 4 elements will be imported from the previous preprocessing stage\n","called preprocessor.py\n","\n","2. Training block: here there is also the option of continuing from a previously saved model\n","using the checkpoint operation\n","3. Saving the generated model for next stage\"\"\"\n","\n","#Checking GPU opertion\n","import tensorflow as tf\n","tf.test.gpu_device_name() #debe dar \"/device: GPU:0\" otherwise \"\"\n","\n","from re import S\n","import music21 as m21\n","import pandas as pd\n","import numpy as np\n","import os\n","import keras\n","#from keras.utils import to_categorical\n","from tensorflow.keras.utils import to_categorical\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","#For training unit\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, LSTM\n","from keras.callbacks import ModelCheckpoint\n","from keras.layers import Activation\n","from keras.layers import BatchNormalization as BatchNorm\n","from keras.callbacks import ModelCheckpoint\n","from keras.models import load_model\n","from keras.utils import np_utils\n","\n","import configure\n","\n","import json\n","import glob\n","import pickle as pkl\n","\n","import logging\n","\n","logging.getLogger(\"tensorflow\").setLevel(logging.ERROR) #for avoiding annoying tf warnings\n","\n","\n","\n","###LOAD THE DATA AND PARAMETERS\n","\n","data_path = configure.TRAINING_DATA_PATH\n","\n","def data_load(data_path):\n","    \"\"\"Loads the training data and parameters using pickle\n","args: data_path: location of the pkl file containing the data\n","called training_data.pkl and unpacks all the data\n","\"\"\"\n","    with open(data_path, \"rb\") as f:\n","        X, X_f, y, vocab_length, all_keys_vocabulary = pkl.load(f)\n","    print(\"Loaded data summary:\")\n","    print(\"=======================\")\n","    print(\"Number of training examples:\", len(X))\n","    print(\"Processed input data size:\", X_f.shape)\n","    print(\"Target data size:\", y.shape)\n","    print(\"Nr of target classes:\", vocab_length)\n","    return X, X_f, y, vocab_length, all_keys_vocabulary\n","\n","\n","#####TRAINING UNIT\n","OUTPUT_UNITS = configure.OUTPUT_UNITS #to be obtained as vocab_size variable from generate_training_sequences\n","NUM_UNITS = configure.NUM_UNITS #Hidden layer units\n","LOSS = configure.LOSS\n","LEARNING_RATE = configure.LEARNING_RATE\n","EPOCHS = configure.EPOCHS\n","BATCH_SIZE = configure.BATCH_SIZE\n","SAVED_MODEL_NAME = configure.SAVED_MODEL_NAME\n","\n","\n","def train_model(model, inputs, targets, model_name = SAVED_MODEL_NAME,\n"," batch_size = BATCH_SIZE, epochs = EPOCHS):\n","    \"\"\"Train and save model\"\"\"\n","    filepath = \"/content/drive/MyDrive/maeGenerator22/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n","    checkpoint = ModelCheckpoint(\n","        filepath,\n","        monitor='loss',\n","        verbose=0,\n","        save_best_only=True,\n","        mode='min'\n","    )\n","    callbacks_list = [checkpoint]\n","   \n","    model.fit(inputs, targets,\n"," batch_size = BATCH_SIZE, epochs = epochs, callbacks= callbacks_list)\n","\n","    #Save the model\n","    model.save(model_name)\n","    print(\"Training complete!\")\n","\n","    return model\n","\n","def build_the_model(inputs, vocab_size):\n","    \"\"\"Create the architecture of the network\"\"\"\n","    model = Sequential()\n","    model.add(LSTM(512, \n","    input_shape = (inputs.shape[1], inputs.shape[2]),\n","    recurrent_dropout= 0.3,\n","    return_sequences= True))\n","\n","    model.add(LSTM(512,recurrent_dropout= 0.3, return_sequences= True))\n","    model.add(LSTM(512))\n","    model.add(BatchNorm())\n","    model.add(Dropout(0.3))\n","    model.add(Dense(256))\n","    model.add(Activation(\"relu\"))\n","    model.add(BatchNorm())\n","    model.add(Dropout(0.3))\n","    model.add(Dense(vocab_size))\n","    model.add(Activation(\"softmax\"))\n","    \n","    model.compile(loss =\"categorical_crossentropy\", optimizer = \"rmsprop\")\n","    #model.summary()\n","    return model\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"RZ_UGUYqBak1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded data summary:\n","=======================\n","Number of training examples: 200674\n","Processed input data size: (200674, 100, 1)\n","Target data size: (200674, 444)\n","Nr of target classes: 444\n","Epoch 1/30\n","   3/1568 [..............................] - ETA: 3:40:53 - loss: 6.3734"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-5-f2bc2b9d24ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_keys_vocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_the_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSAVED_MODEL_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;31m#prediction = generate_notes(X, SAVED_MODEL_NAME, list(set(notes)), vocab_length, 0.9)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#convert_to_midi(prediction)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-1-57efbb76dcce>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, inputs, targets, model_name, batch_size, epochs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     model.fit(inputs, targets,\n\u001b[1;32m--> 104\u001b[1;33m  batch_size = BATCH_SIZE, epochs = epochs, callbacks= callbacks_list)\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;31m#Save the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1156\u001b[0m                 _r=1):\n\u001b[0;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1158\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1159\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3024\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1961\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#TESTING\n","\n","if __name__ == \"__main__\":\n","    #ORIGINAL SEQUENCE\n","    X, X_f, y, vocab_length, all_keys_vocabulary = data_load(data_path)\n","    model = build_the_model(X_f, vocab_length)\n","    model = train_model(model, inputs = X_f, targets = y, model_name = SAVED_MODEL_NAME,batch_size = BATCH_SIZE, epochs = EPOCHS)\n","    print(\"Training complete!\")\n","\n","    #LOADING A CHECKPOINT AND CONTINUE\n","    # TEST_EPOCHS = 30 #Test for 2 epochs\n","    # X, X_f, y, vocab_length = data_load(data_path)\n","    # new_model = load_model(\"/content/drive/MyDrive/maeGenerator22/weights-7PM.hdf5\")\n","    # model = train_model(new_model, inputs = X_f, targets = y, model_name = SAVED_MODEL_NAME,batch_size = BATCH_SIZE, epochs = TEST_EPOCHS)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbR-yZG2DfWl"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNzqgNoH+87wOUaEG5ka0lA","collapsed_sections":[],"mount_file_id":"1sCTqyJSEgsfS6_9XfJhr9nNprekC0oJX","name":"LSTMtrain.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}
